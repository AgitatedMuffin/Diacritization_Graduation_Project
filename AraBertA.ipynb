{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ca0ab2",
   "metadata": {},
   "source": [
    "# Arabic Diacritization using RoBerta Transformers implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8143710",
   "metadata": {},
   "source": [
    "## First we need a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tokenizers\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753fdbb",
   "metadata": {},
   "source": [
    "### Getting all the arabic combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074abdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_letters = ['أ','ة','إ','ؤ','آ','ا','ب','ت', 'ث','ج','ح','خ','د','ذ','ر','ز','س','ش','ص','ض','ط','ظ','ع','غ','ف','ق','ك','ل','م','ن','ه','و','ي','ئ','ئ','ء']\n",
    "arabic_diac = [\"َ\",\"ً\",\"ِ\",\"ٍ\",\"ُ\",\"ٌ\",\"ْ\",\"َّ\",\"ِّ\",\"ُّ\"]\n",
    "\n",
    "arabic_combinations= [];\n",
    "for letter in arabic_letters:\n",
    "    for diac in arabic_diac:\n",
    "        arabic_combinations.append(letter+diac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58088a",
   "metadata": {},
   "source": [
    "### Adding the tokens \n",
    "(Note the tokenizer won't save the tokens/ this is a tokenizer bug at the time of writing this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636454ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(arabic_combinations)\n",
    "tokenizer.add_tokens(arabic_letters)\n",
    "tokenizer.add_special_tokens([\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8135f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model('./Downloads/AraBertA/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbcb3c",
   "metadata": {},
   "source": [
    "### Testing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5d5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"الَسَلَام\").ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb07f8",
   "metadata": {},
   "source": [
    "Looks Like it's working great"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c3e95",
   "metadata": {},
   "source": [
    "## Now for the Roberta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d944b2c",
   "metadata": {},
   "source": [
    "### Making the tokenizer a RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73560bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained('./Downloads/AraBertA/')\n",
    "\n",
    "roberta_tokenizer.add_tokens(arabic_diac)\n",
    "roberta_tokenizer.add_tokens(\" \")\n",
    "roberta_tokenizer.add_tokens(arabic_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer.from_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd4971",
   "metadata": {},
   "source": [
    "### Creating a new instance from the Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068535a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig,RobertaForMaskedLM\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=len(roberta_tokenizer.get_vocab()),\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c07d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54263bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer.tokenize('السَلام عَليكُم')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c898db1",
   "metadata": {},
   "source": [
    "Looks Like it's working as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfb838",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063258d",
   "metadata": {},
   "source": [
    "### Importing all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paths_train = [str(x) for x in Path(\"./Downloads/tashkeela_processing/tashkeela_train/\").glob(\"*.txt\")]\n",
    "paths_eval = [str(x) for x in Path(\"./Downloads/tashkeela_processing/tashkeela_val/\").glob(\"*.txt\")]\n",
    "paths_test = [str(x) for x in Path(\"./Downloads/tashkeela_processing/tashkeela_test/\").glob(\"*.txt\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7eeccf",
   "metadata": {},
   "source": [
    "### The Datasets library is a time-saving library from hugging face and it does it's job pretty well in keeping everything organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3474ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "from datasets import load_dataset,DatasetDict\n",
    "\n",
    "train_dataset = load_dataset('text',data_files=paths_train, split='train')\n",
    "eval_dataset = load_dataset('text',data_files=paths_eval, split='train')\n",
    "test_dataset = load_dataset('text',data_files=paths_test, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98025f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming the final DataSetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'eval': eval_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabc9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fdae9",
   "metadata": {},
   "source": [
    "## In this next part we will try to augment the data by creating different variants of each sentence with random diacritics stripped from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee76bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randint\n",
    "\n",
    "def create_diacritization_variants(example: str) -> list:\n",
    "    \"\"\"\n",
    "    Creates a random number of variants of any string\n",
    "    with randomly stripped diacritizations. Each new variant is more stripped down.\n",
    "    \"\"\"\n",
    "    words= list(example.split(\" \"))\n",
    "    arabic_diac = [\"َ\",\"ً\",\"ِ\",\"ٍ\",\"ُ\",\"ٌ\",\"ْ\",\"َّ\",\"ِّ\",\"ُّ\"]\n",
    "    list_of_example_variants=[]\n",
    "    modified_example = words\n",
    "    for iteration in range(randint(2,5)):\n",
    "        for i in range(len(words)) :\n",
    "            removed_diac=random.choice(arabic_diac)\n",
    "            value=randint(1,10)\n",
    "            modified_example[i] = modified_example[i].replace(removed_diac,'')\n",
    "        list_of_example_variants.append(' '.join(modified_example))\n",
    "    return list_of_example_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03ece2",
   "metadata": {},
   "source": [
    "### Augmentiong the dataset\n",
    "Careful as this cell takes a very long time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960795e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "progress_bar = tqdm(range(ds['train'].num_rows))\n",
    "\n",
    "augmented_train_data = []\n",
    "for sentence in ds['train']['text']:\n",
    "    progress_bar.update(1)\n",
    "    augmented_train_data = augmented_train_data + create_diacritization_variants(sentence)\n",
    "\n",
    "augmented_train_dict = {'text': augmented_train_data}\n",
    "augmented_train_dataset = Dataset.from_dict(augmented_train_dict).shuffle()\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "progress_bar = tqdm(range(ds['eval'].num_rows))\n",
    "\n",
    "augmented_eval_data = []\n",
    "for sentence in ds['train']['text']:\n",
    "    progress_bar.update(1)\n",
    "    augmented_eval_data = augmented_eval_data + create_diacritization_variants(sentence)\n",
    "\n",
    "augmented_eval_dict = {'text': augmented_eval_data}\n",
    "augmented_eval_dataset = Dataset.from_dict(augmented_train_dict).shuffle()\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "progress_bar = tqdm(range(ds['eval'].num_rows))\n",
    "\n",
    "augmented_test_data = []\n",
    "for sentence in ds['train']['text']:\n",
    "    progress_bar.update(1)\n",
    "    augmented_test_data = augmented_test_data + create_diacritization_variants(sentence)\n",
    "\n",
    "augmented_test_dict = {'text': augmented_test_data}\n",
    "augmented_test_dataset = Dataset.from_dict(augmented_test_dict).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_ds = DatasetDict{\n",
    "    'train': augmented_train_dataset,\n",
    "    'eval': augmented_eval_dataset,\n",
    "    'test': augmented_test_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    return roberta_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "encoded_train_dataset = ds['train'].map(encode, batched = True);\n",
    "encoded_eval_dataset = ds['eval'].map(encode, batched = True);\n",
    "encoded_test_dataset = ds['test'].map(encode, batched = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafad39c",
   "metadata": {},
   "source": [
    " ### Now we need to explicity specify for the data collator not to mask any letters, only the diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475504a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForLanguageModeling:\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
    "    are not all of the same length.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        mlm (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to use masked language modeling. If set to :obj:`False`, the labels are the same as the\n",
    "            inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for\n",
    "            non-masked tokens and the value to predict for the masked token.\n",
    "        mlm_probability (:obj:`float`, `optional`, defaults to 0.15):\n",
    "            The probability with which to (randomly) mask tokens in the input, when :obj:`mlm` is set to :obj:`True`.\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For best performance, this data collator should be used with a dataset having items that are dictionaries or\n",
    "        BatchEncoding, with the :obj:`\"special_tokens_mask\"` key, as returned by a\n",
    "        :class:`~transformers.PreTrainedTokenizer` or a :class:`~transformers.PreTrainedTokenizerFast` with the\n",
    "        argument :obj:`return_special_tokens_mask=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.mlm and self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n",
    "                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n",
    "            )\n",
    "\n",
    "    def __call__(\n",
    "        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], (dict, BatchEncoding)):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\"input_ids\": _collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)}\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def mask_tokens(\n",
    "        self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            my_mask = inputs> 15\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool) | my_mask\n",
    "            \n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "tokenizer=roberta_tokenizer, mlm=True, mlm_probability=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52931f86",
   "metadata": {},
   "source": [
    "## Defining the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./AraBertA/model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size= 8,\n",
    "    per_device_eval_batch_size= 8,\n",
    "    logging_steps=10,\n",
    "    save_steps=5,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, data_collator=data_collator ,train_dataset=encoded_eval_dataset, eval_dataset= encoded_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e919bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe306d2",
   "metadata": {},
   "source": [
    "### Now it's finally time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad693db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from torch import cuda\n",
    "cuda.empty_cache()\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b1917",
   "metadata": {},
   "source": [
    "# Now to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_diac_dataset = load_dataset('text',data_files=['./AraBertA/data/old_data/test_no_diac.txt'], split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea06837",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0;\n",
    "masked_sentence_list = [];\n",
    "for char in test_eval_dataset['text'][0]:\n",
    "    temp = list(test_no_diac_dataset['text'][0])\n",
    "    temp[i] = '<mask>'\n",
    "    temp = ''.join(temp)\n",
    "    i= i+1\n",
    "    masked_sentence_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee71df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = transformers.pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./AraBertA/model_4/checkpoint-19900/\",\n",
    "    tokenizer=roberta_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask('وَقُلْتُمْ: «لاَ بَلْ عَلَى خَيْل نَهْر<mask>». لِذلِكَ تَهْبُونَ.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19868d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_of_pipeline = [];\n",
    "for sentence in masked_sentence_list:\n",
    "    tmp = fill_mask(sentence)\n",
    "    output_of_pipeline.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71644b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text = [];\n",
    "for i in range(len(output_of_pipeline)):\n",
    "    predicted_text.append(output_of_pipeline[i][0]['token_str'])\n",
    "    \n",
    "predicted_text = ''.join(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_dataset[0]['input_ids']\n",
    "predicted_text = [];\n",
    "for i in range(len(output_of_pipeline)):\n",
    "    predicted_text.append(output_of_pipeline[i][0]['token_str'])\n",
    "    \n",
    "encoded_predicted_text = roberta_tokenizer.encode(''.join(predicted_text))\n",
    "\n",
    "summation = 0;\n",
    "for i in range(256):\n",
    "    if encoded_predicted_text[i] == encoded_test_dataset[0]['input_ids'][i]:\n",
    "        summation = summation +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c289149",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = summation/256\n",
    "accuracy*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
